{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOvvWAVTkMR7"
   },
   "source": [
    "# Eager Few Shot Object Detection Colab\n",
    "\n",
    "Welcome to the Eager Few Shot Object Detection Colab --- in this colab we demonstrate fine tuning of a (TF2 friendly) RetinaNet architecture on very few examples of a novel class after initializing from a pre-trained COCO checkpoint.\n",
    "Training runs in eager mode.\n",
    "\n",
    "Estimated time to run through this colab (with GPU): < 5 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPs64QA1Zdov"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone --depth 1 https://github.com/tensorflow/models\n",
    "\n",
    "# %cd  \"C:\\Users\\sky66\\Downloads\\models\\research\" (cd to this folder)\n",
    "# jupyter notebook\n",
    "\n",
    "# conda install tensorflow=2.5.0=gpu_py39h7dc34a2_0\n",
    "# conda uninstall tensorflow\n",
    "# pip install \"tensorflow==2.7.0\"\n",
    "\n",
    "# pip uninstall pyparsing -y\n",
    "# pip install pyparsing==2.4.2\n",
    "\n",
    "# pip uninstall pyyaml -y\n",
    "# pip install pyyaml==5.1\n",
    "\n",
    "# protoc object_detection/protos/*.proto --python_out=.\n",
    "# python object_detection/packages/tf2/setup.py build\n",
    "# python object_detection/packages/tf2/setup.py install\n",
    "\n",
    "# conda list\n",
    "\n",
    "# 參考資料\n",
    "# https://www.jianshu.com/p/f8ffbf18c312\n",
    "# https://stackoverflow.com/questions/68737130/error-while-import-keras-attributeerror-module-tensorflow-compat-v2-interna\n",
    "# https://github.com/tensorflow/tensorflow/issues/53060\n",
    "# https://medium.com/ching-i/win10-%E5%AE%89%E8%A3%9D-cuda-cudnn-%E6%95%99%E5%AD%B8-c617b3b76deb\n",
    "# https://github.com/tensorflow/tensorflow/issues/52988\n",
    "# https://cppsecrets.com/users/17211410511511610510710997106117109100971144964103109971051084699111109/Python-tqdmsetpostfixstr.php\n",
    "# https://www.delftstack.com/zh-tw/howto/python/python-print-flush/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZcqD4NLdnf4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import random\n",
    "import io\n",
    "import imageio\n",
    "import glob\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from six import BytesIO\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display, Javascript\n",
    "from IPython.display import Image as IPyImage\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageColor\n",
    "from PIL import ImageDraw\n",
    "from PIL import ImageFont\n",
    "from PIL import ImageOps\n",
    "from IPython.display import display as ds\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSaXL28TZfk1"
   },
   "source": [
    "# Custom Image data\n",
    "\n",
    "We will start with some toy (literally) data consisting of 6 images of two insect.  Note that the [coco](https://cocodataset.org/#explore) dataset contains a number of animals, but notably, it does *not* contain some datas , so this is a novel class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbox.csv 是由 Get_BBox.ipynb 建立的。\n",
    "name = [0]\n",
    "bbox = [0]\n",
    "with open(\"open_images_v6_bbox.csv\", \"r\", encoding=\"utf8\") as f:\n",
    "    f = iter(f)\n",
    "    info = next(f).strip().split(\",\")\n",
    "    ie = 0\n",
    "    try:\n",
    "        while ie<4170:\n",
    "        #while 1:\n",
    "            ie += 1\n",
    "            e = next(f).strip().split(\",\")\n",
    "            content = [float(i) for i in [e[3], e[1], e[4], e[2]]]\n",
    "            name.append(e[0])\n",
    "            bbox.append(content)\n",
    "    except:\n",
    "        pass\n",
    "name.pop(0)\n",
    "bbox.pop(0)\n",
    "for e in range(len(name)):\n",
    "    name[e] = name[e]+\".jpg\"\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [0]\n",
    "for e in name:\n",
    "    if e!=c[-1]:\n",
    "        c.append(e)\n",
    "c.pop(0)\n",
    "print(len(c))\n",
    "del c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"C:\\Users\\sky66\\fiftyone\\open-images-v6\\validation\\data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in name:\n",
    "#     if cv2.imread(image_path+\"\\\\\"+i).shape != (640, 640, 3):\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map= {0: \"Person\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy from Train_Unet_Model.ipynb\n",
    "def random_image_process_v1(img_np, box, recorded, ids): \n",
    "    # image format cv2 (0~255)\n",
    "    # box format np.array shape(4,)\n",
    "    img = img_np\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    # 用隨機數判斷是否旋轉圖片，增強訓練集 (因為訓練用的圖片是正方形，所以可以這樣做。)            \n",
    "    if not(recorded):\n",
    "        global now_point\n",
    "        ctr = np.random.randint(0,6)\n",
    "        now_point = [ids, ctr]\n",
    "    else:\n",
    "        ctr = now_point[1]\n",
    "    # print(ctr)\n",
    "    img = color(img, case[ctr])\n",
    "    \n",
    "    if ctr==0:\n",
    "        new_xmin = xmin\n",
    "        new_xmax = xmax\n",
    "        new_ymin = ymin\n",
    "        new_ymax = ymax\n",
    "    elif ctr==1:\n",
    "        # 順時針旋轉90度               \n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "        new_xmin = 1-ymax\n",
    "        new_xmax = 1-ymin\n",
    "        new_ymin = xmin\n",
    "        new_ymax = xmax\n",
    "    elif ctr==2:\n",
    "        # 旋轉180度\n",
    "        img = cv2.rotate(img, cv2.ROTATE_180)\n",
    "        new_xmin = 1-xmax\n",
    "        new_xmax = 1-xmin\n",
    "        new_ymin = 1-ymax\n",
    "        new_ymax = 1-ymin      \n",
    "    elif ctr==3:\n",
    "        # 順時針旋轉270度(逆時針旋轉90度)\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        new_xmin = ymin\n",
    "        new_xmax = ymax\n",
    "        new_ymin = 1-xmax\n",
    "        new_ymax = 1-xmin\n",
    "    elif ctr==4:\n",
    "        # 水平翻轉\n",
    "        img = cv2.flip(img, 1)\n",
    "        new_xmin = 1-xmax\n",
    "        new_xmax = 1-xmin\n",
    "        new_ymin = ymin\n",
    "        new_ymax = ymax\n",
    "    elif ctr==5:\n",
    "        # 上下翻轉\n",
    "        img = cv2.flip(img, 0)\n",
    "        new_xmin = xmin\n",
    "        new_xmax = xmax\n",
    "        new_ymin = 1-ymax\n",
    "        new_ymax = 1-ymin\n",
    "    #Add to the batch data.\n",
    "    # 對圖片做亮度、對比度處理，增強訓練集\n",
    "    # 參考網站 https://www.wongwonggoods.com/python/python_opencv/opencv-modify-contrast/\n",
    "    ctr = np.random.randint(1,101)\n",
    "    if ctr>=35: # 65% 機率調整對比度\n",
    "        brightness = 0\n",
    "        contrast = np.random.randint(1,70) # - 減少對比度/+ 增加對比度  變動幅度 -50~+50    \n",
    "        if ctr>=25:\n",
    "            contrast = contrast*-1\n",
    "        B = brightness / 255.0\n",
    "        c = contrast / 255.0 \n",
    "        k = math.tan((45 + 44 * c) / 180 * math.pi)                \n",
    "        img = (img - 127.5 * (1 - B)) * k + 127.5 * (1 + B)\n",
    "        # 所有值必須介於 0~255 之間，超過255 = 255，小於 0 = 0\n",
    "        img = np.clip(img, 0, 255)  \n",
    "    ctr = np.random.randint(1,101)    \n",
    "    if ctr>=35: # 65% 機率調整亮度\n",
    "        phi = np.random.randint(5,16)/10 # phi>1 減少亮度  phi<1 增加亮度 phi:0.5~1.5\n",
    "        img = (img/255)**phi\n",
    "        img = np.clip(img*255, 0, 255)  \n",
    "    #if ctr==1 : # 50% 機率黑白顛倒\n",
    "    #    img = 255-img        \n",
    "    box = np.array([new_ymin, new_xmin, new_ymax, new_xmax], dtype=\"float16\")\n",
    "    # output img format cv2 GRAY\n",
    "    return img, box\n",
    "\n",
    "# Copy from Train_Unet_Model.ipynb\n",
    "def image_process_default(img_np, box): \n",
    "    # image format cv2 (0~255)\n",
    "    # box format np.array shape(4,)\n",
    "    img = img_np\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    new_xmin = xmin\n",
    "    new_xmax = xmax\n",
    "    new_ymin = ymin\n",
    "    new_ymax = ymax\n",
    "    #Add to the batch data.\n",
    "    # 對圖片做亮度、對比度處理，增強訓練集\n",
    "    # 參考網站 https://www.wongwonggoods.com/python/python_opencv/opencv-modify-contrast/\n",
    "    ctr = np.random.randint(1,101)\n",
    "    if ctr>=35: # 65% 機率調整對比度\n",
    "        brightness = 0\n",
    "        contrast = np.random.randint(1,70) # - 減少對比度/+ 增加對比度  變動幅度 -50~+50    \n",
    "        if ctr>=25:\n",
    "            contrast = contrast*-1\n",
    "        B = brightness / 255.0\n",
    "        c = contrast / 255.0 \n",
    "        k = math.tan((45 + 44 * c) / 180 * math.pi)                \n",
    "        img = (img - 127.5 * (1 - B)) * k + 127.5 * (1 + B)\n",
    "        # 所有值必須介於 0~255 之間，超過255 = 255，小於 0 = 0\n",
    "        img = np.clip(img, 0, 255)  \n",
    "    ctr = np.random.randint(1,101)    \n",
    "    if ctr>=35: # 65% 機率調整亮度\n",
    "        phi = np.random.randint(5,16)/10 # phi>1 減少亮度  phi<1 增加亮度 phi:0.5~1.5\n",
    "        img = (img/255)**phi\n",
    "        img = np.clip(img*255, 0, 255)  \n",
    "    #if ctr==1 : # 50% 機率黑白顛倒\n",
    "    #    img = 255-img        \n",
    "    box = np.array([new_ymin, new_xmin, new_ymax, new_xmax], dtype=\"float16\")\n",
    "    # output img format cv2 GRAY\n",
    "    return img, box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_path = r\"C:\\Users\\sky66\\fiftyone\\coco-2017\\raw\\nlp\"\n",
    "# test_id = 0\n",
    "# test_img = cv2.imread(test_path+\"\\\\\"+name[test_id], cv2.IMREAD_GRAYSCALE)\n",
    "# process_img, process_box = random_image_process(test_img, bbox[test_id])\n",
    "# draw_test = draw_bounding_box_on_image(process_img, *process_box, display_str_list=[\"Drawing Test\"])\n",
    "# ds(draw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = {0:\"RGB\",1:\"RBG\",2:\"BGR\",3:\"BRG\",4:\"GRB\",5:\"GBR\"}\n",
    "def color(img_np_, mode):\n",
    "    img_np = img_np_.copy()\n",
    "    a = img_np_\n",
    "    # img_np is BGR cv2 default mode.\n",
    "    # 6!，R、G、B三種元素，6階、6種可能。\n",
    "    if  mode==\"RGB\":\n",
    "        img_np[:,:,0:1] = a[:,:,2:3] #R\n",
    "        img_np[:,:,1:2] = a[:,:,1:2] #G\n",
    "        img_np[:,:,2:3] = a[:,:,0:1] #B\n",
    "    elif mode==\"RBG\":\n",
    "        img_np[:,:,0:1] = a[:,:,2:3] #R\n",
    "        img_np[:,:,1:2] = a[:,:,0:1] #B\n",
    "        img_np[:,:,2:3] = a[:,:,1:2] #G\n",
    "    elif mode==\"BGR\":\n",
    "        img_np[:,:,0:1] = a[:,:,0:1] #B\n",
    "        img_np[:,:,1:2] = a[:,:,1:2] #G\n",
    "        img_np[:,:,2:3] = a[:,:,2:3] #R\n",
    "    elif mode==\"BRG\":\n",
    "        img_np[:,:,0:1] = a[:,:,0:1] #B\n",
    "        img_np[:,:,1:2] = a[:,:,2:3] #R\n",
    "        img_np[:,:,2:3] = a[:,:,1:2] #G\n",
    "    elif mode==\"GRB\":\n",
    "        img_np[:,:,0:1] = a[:,:,1:2] #G\n",
    "        img_np[:,:,1:2] = a[:,:,2:3] #R\n",
    "        img_np[:,:,2:3] = a[:,:,0:1] #B\n",
    "    elif mode==\"GBR\":\n",
    "        img_np[:,:,0:1] = a[:,:,1:2] #G\n",
    "        img_np[:,:,1:2] = a[:,:,0:1] #B\n",
    "        img_np[:,:,2:3] = a[:,:,2:3] #R\n",
    "    return img_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_train(point, batch_size, total):\n",
    "    batch_size -= 1 # Inclube start point  \n",
    "    if(point+batch_size>total):\n",
    "        f = point+batch_size-total\n",
    "        g = point+batch_size+1-f\n",
    "        return list(range(point,g))+list(range(0, f))\n",
    "    else:\n",
    "        g = point+batch_size+1\n",
    "        return list(range(point,g))\n",
    "\n",
    "def flag(point, batch_size, data_size):\n",
    "    #Choose random indice for later picking.\n",
    "    rnd_ind = on_train(point, batch_size, data_size-1)\n",
    "    if((point + batch_size) >= data_size):\n",
    "        point = point + batch_size - data_size\n",
    "    else:\n",
    "        point = point + batch_size \n",
    "    return  rnd_ind, point   \n",
    "\n",
    "img_size = [640, 640, 1]\n",
    "def data_generator(images_path, image_ids, bbox, batch_size, image_process):\n",
    "    global train_point\n",
    "    \n",
    "    data_size = len(image_ids)\n",
    "    while True:     \n",
    "        rnd_ind, point = flag(train_point, batch_size, data_size)\n",
    "        train_point = point\n",
    "        imgs = []\n",
    "        boxes = []\n",
    "        for i in rnd_ind:\n",
    "            img_id = image_ids[i]\n",
    "            box = bbox[i]\n",
    "            #Load/resize images.\n",
    "            img = cv2.imread(images_path +\"\\\\\" + img_id)\n",
    "            if img_id==now_point[0]:\n",
    "                img, box = image_process(img, box, True, img_id)\n",
    "            else:\n",
    "                img, box = image_process(img, box, False, img_id)\n",
    "            img = tf.expand_dims(tf.convert_to_tensor(img, dtype=tf.float32) , axis=0)\n",
    "            box = tf.expand_dims(tf.convert_to_tensor(box, dtype=tf.float32) , axis=0)\n",
    "            imgs.append(img)  \n",
    "            boxes.append(box)\n",
    "        yield imgs, boxes\n",
    "        \n",
    "def defalut_data_generator(images_path, image_ids, bbox, batch_size, image_process):\n",
    "    global train_point\n",
    "    \n",
    "    data_size = len(image_ids)\n",
    "    while True:     \n",
    "        rnd_ind, point = flag(train_point, batch_size, data_size)\n",
    "        train_point = point\n",
    "        imgs = []\n",
    "        boxes = []\n",
    "        for i in rnd_ind:\n",
    "            img_id = image_ids[i]\n",
    "            box = bbox[i]\n",
    "            #Load/resize images.\n",
    "            img = load_img(images_path +\"\\\\\" + img_id, target_size=img_size[:-1], color_mode = 'grayscale')\n",
    "            img = img_to_array(img)\n",
    "            img, box = image_process(img, box)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "            img = tf.expand_dims(tf.convert_to_tensor(img, dtype=tf.float32) , axis=0)\n",
    "            box = tf.expand_dims(tf.convert_to_tensor(box, dtype=tf.float32) , axis=0)\n",
    "            imgs.append(img)  \n",
    "            boxes.append(box)\n",
    "        yield imgs, boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_point = 0\n",
    "# val_point = 0\n",
    "# now_point = [\"file jpg\", 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = data_generator(image_path, name, bbox, 5, random_image_process_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i, b = next(s)\n",
    "# plt.imshow(i[0].numpy()[0]/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cbKXmQoxcUgE"
   },
   "source": [
    "# Annotate images with bounding boxes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dqb_yjAo3cO_"
   },
   "source": [
    "# Prepare data for training\n",
    "\n",
    "Below we add the class annotations (for simplicity, we assume a single class in this colab; though it should be straightforward to extend this to handle multiple classes).  We also convert everything to the format that the training\n",
    "loop below expects (e.g., everything converted to tensors, classes converted to one-hot representations, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_indexed_groundtruth_classes = tf.convert_to_tensor(0)\n",
    "gt_classes = tf.expand_dims(tf.one_hot(zero_indexed_groundtruth_classes, num_classes), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghDAsqfoZvPh"
   },
   "source": [
    "# Create model and restore weights for all but last layer\n",
    "\n",
    "In this cell we build a single stage detection architecture (RetinaNet) and restore all but the classification layer at the top (which will be automatically randomly initialized).\n",
    "\n",
    "For simplicity, we have hardcoded a number of things in this colab for the specific RetinaNet architecture at hand (including assuming that the image size will always be 640x640), however it is not difficult to generalize to other model configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9J16r3NChD-7"
   },
   "outputs": [],
   "source": [
    "# Download the checkpoint and put it into models/research/object_detection/test_data/\n",
    "# \n",
    "# !wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "# !tar -xf ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.tar.gz\n",
    "# !mv ssd_resnet50_v1_fpn_640x640_coco17_tpu-8/checkpoint models/research/object_detection/test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyT4BUbaMeG-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# num_classes = 1\n",
    "# print('Building model and restoring weights for fine-tuning...', flush=True)\n",
    "# pipeline_config = 'models/research/object_detection/configs/tf2/ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config'\n",
    "# checkpoint_path = 'models/research/object_detection/test_data/checkpoint/ckpt-0'\n",
    "\n",
    "# pipeline_config = r\"C:\\Users\\sky66\\Downloads\\models\\research\\ckpt\\ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config\"\n",
    "pipeline_config = r\"C:\\Users\\sky66\\Downloads\\models\\research\\ckpt\\pipeline.config\"\n",
    "checkpoint_path = r\"C:\\Users\\sky66\\Downloads\\models\\research\\ckpt\"\n",
    "\n",
    "# Load pipeline config and build a detection model.\n",
    "#\n",
    "# Since we are working off of a COCO architecture which predicts 90\n",
    "# class slots by default, we override the `num_classes` field here to be just\n",
    "# one (for our new rubber ducky class).\n",
    "configs = config_util.get_configs_from_pipeline_file(pipeline_config)\n",
    "model_config = configs['model']\n",
    "model_config.ssd.num_classes = num_classes\n",
    "# model_config.ssd.freeze_batchnorm = True\n",
    "detection_model = model_builder.build(\n",
    "      model_config=model_config, is_training=True)\n",
    "\n",
    "# Set up object-based checkpoint restore --- RetinaNet has two prediction\n",
    "# `heads` --- one for classification, the other for box regression.  We will\n",
    "# restore the box regression head but initialize the classification head\n",
    "# from scratch (we show the omission below by commenting out the line that\n",
    "# we would add if we wanted to restore both heads)\n",
    "\n",
    "# Line : _prediction_heads=detection_model._box_predictor._prediction_heads \n",
    "#        will determined : the classification head that we will restore or not.\n",
    "# 在第一次訓練，因為 num_classes 不同、載入的 ckpt也是不同的 num_classes，所以我們會把它註解起來，不執行它。\n",
    "# 在第一次訓練過後，我們就部會註解起來，會去執行它了。(因為用新儲存的 checkpoint)\n",
    "fake_box_predictor = tf.compat.v2.train.Checkpoint(\n",
    "    _base_tower_layers_for_heads=detection_model._box_predictor._base_tower_layers_for_heads,\n",
    "    _prediction_heads=detection_model._box_predictor._prediction_heads,\n",
    "    _box_prediction_head=detection_model._box_predictor._box_prediction_head,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "fake_model = tf.compat.v2.train.Checkpoint(\n",
    "          _feature_extractor=detection_model._feature_extractor,\n",
    "          _box_predictor=fake_box_predictor)\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=fake_model)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, directory=checkpoint_path, max_to_keep=5)\n",
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "# ckpt.restore(checkpoint_path).expect_partial()\n",
    "\n",
    "# Run model through a dummy image so that variables are created\n",
    "image, shapes = detection_model.preprocess(tf.zeros([1, 640, 640, 3]))\n",
    "prediction_dict = detection_model.predict(image, shapes)\n",
    "_ = detection_model.postprocess(prediction_dict, shapes)\n",
    "print('Weights restored!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyT4BUbaMeG-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pipeline_config = r\"C:\\Users\\sky66\\Downloads\\models\\research\\my_model\\ssd_resnet50_v1_fpn_640x640_coco17_tpu-8.config\"\n",
    "# checkpoint_path = r\"C:\\Users\\sky66\\Downloads\\models\\research\\my_model\\new_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pCkWmdoZZ0zJ"
   },
   "source": [
    "# Custom training loop\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyHoF4mUrv5-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(True)\n",
    "\n",
    "# Select variables in top layers to fine-tune.\n",
    "trainable_variables = detection_model.trainable_variables\n",
    "to_fine_tune = []\n",
    "prefixes_to_train = [\n",
    "    'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalBoxHead',\n",
    "    'WeightSharedConvolutionalBoxPredictor/WeightSharedConvolutionalClassHead']\n",
    "for var in trainable_variables:\n",
    "    if any([var.name.startswith(prefix) for prefix in prefixes_to_train]):\n",
    "        to_fine_tune.append(var)\n",
    "\n",
    "# Set up forward + backward pass for a single train step.\n",
    "def get_model_train_step_function(model, optimizer, vars_to_fine_tune):\n",
    "    \"\"\"Get a tf.function for training step.\"\"\"\n",
    "    \n",
    "    # Use tf.function for a bit of speed.\n",
    "    # Comment out the tf.function decorator if you want the inside of the\n",
    "    # function to run eagerly.\n",
    "    @tf.function\n",
    "    def train_step_fn(image_tensors,\n",
    "                      groundtruth_boxes_list,\n",
    "                      groundtruth_classes_list):\n",
    "        \"\"\"A single training iteration.\n",
    "        \n",
    "        Args:\n",
    "            image_tensors: A list of [1, height, width, 3] Tensor of type tf.float32.\n",
    "              Note that the height and width can vary across images, as they are\n",
    "              reshaped within this function to be 640x640.\n",
    "            groundtruth_boxes_list: A list of Tensors of shape [N_i, 4] with type\n",
    "              tf.float32 representing groundtruth boxes for each image in the batch.\n",
    "            groundtruth_classes_list: A list of Tensors of shape [N_i, num_classes]\n",
    "              with type tf.float32 representing groundtruth boxes for each image in\n",
    "              the batch.\n",
    "        \n",
    "        Returns:\n",
    "            A scalar tensor representing the total loss for the input batch.\n",
    "        \"\"\"\n",
    "        shapes = tf.constant(batch_size*[[640, 640, 3]], dtype=tf.int32)\n",
    "        model.provide_groundtruth(\n",
    "            groundtruth_boxes_list=groundtruth_boxes_list,\n",
    "            groundtruth_classes_list=groundtruth_classes_list)\n",
    "        with tf.GradientTape() as tape:\n",
    "            preprocessed_images = tf.concat([detection_model.preprocess(image_tensor)[0] for image_tensor in image_tensors], axis=0)\n",
    "            prediction_dict = model.predict(preprocessed_images, shapes)\n",
    "            losses_dict = model.loss(prediction_dict, shapes)\n",
    "            total_loss = losses_dict['Loss/localization_loss'] + losses_dict['Loss/classification_loss']\n",
    "            gradients = tape.gradient(total_loss, vars_to_fine_tune)\n",
    "            optimizer.apply_gradients(zip(gradients, vars_to_fine_tune))\n",
    "        return total_loss\n",
    "\n",
    "    return train_step_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train(batch_size, train):\n",
    "    global now_loss, now_loss_total\n",
    "    image_tensors, gt_boxes_list = next(train)\n",
    "    total_loss = train_step_fn(image_tensors, gt_boxes_list, gt_classes_list)   \n",
    "    now_loss += 1\n",
    "    now_loss_total += float(total_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"%.10f\"%(1e-02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start fine-tuning !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Random Part Datas (Images : 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate\n",
    "# # 這裡可以先用較少的資料快速測試，觀察不同學習率造成的正確率，及模組進步速度。\n",
    "# # 學習率變化(手動調整)\n",
    "# # 如果你是先 \"Train ALL Data\" 再來這邊優化模組，這裡的學習率應當小於 \"Train ALL Data\" 的學習率。\n",
    "# # 你可以先在這邊試試，看看 Total loss，然後再去 \"Train ALL Data\"。\n",
    "# \n",
    "# learning_rate = 1e-3\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "# train_step_fn = get_model_train_step_function(\n",
    "#     detection_model, optimizer, to_fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 隨機抽取出 count_train_images 張圖片，待會用來訓練及觀察。\n",
    "# \n",
    "# batch_size = 5\n",
    "# count_train_images = 25\n",
    "# total_size = round(count_train_images/batch_size)\n",
    "# now_loss = 0\n",
    "# now_loss_total = 0\n",
    "# train_point = 0\n",
    "# gt_classes_list = [gt_classes for i in range(batch_size)]\n",
    "# epochs = 100\n",
    "# history = []\n",
    "# \n",
    "# part_name = []\n",
    "# part_bbox = []\n",
    "# for i in range(count_train_images):\n",
    "#     part_rnd_int = np.random.randint(0,len(name))\n",
    "#     part_name.append(name[part_rnd_int])\n",
    "#     part_bbox.append(bbox[part_rnd_int])\n",
    "#     \n",
    "# part_train = data_generator(image_path, part_name, part_bbox, batch_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     for epoch in range(1,epochs+1):\n",
    "#         now_loss = 0\n",
    "#         now_loss_total = 0\n",
    "#         pbar = tqdm(total = total_size, ncols = 100)\n",
    "#         for one in range(total_size):\n",
    "#             custom_train(batch_size, part_train)\n",
    "#             now_loss_value = now_loss_total/now_loss\n",
    "#             pbar.set_postfix_str(\"epoch %d, Total loss = %.7f\"%(epoch, now_loss_value))\n",
    "#             pbar.update(1)\n",
    "#         pbar.close() \n",
    "#         history.append(now_loss_value)\n",
    "# except:  \n",
    "#     pbar.close()\n",
    "# print(\"Finish\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"optimizer : %s \\nlearning_rate : %s  batch_size : %s  count_train_images : %s\"%(optimizer, learning_rate, batch_size, count_train_images)\n",
    "# plt.plot([i for i in range(1,len(history)+1)], history)\n",
    "# plt.title(text)\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Total Loss\")\n",
    "# plt.show()\n",
    "# plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train ALL Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\"Date\":[], \"Loss\":[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ckpt/history.txt\", \"r\", encoding=\"utf8\") as txt:\n",
    "    for e in txt:\n",
    "        content = e.split(\"#\")\n",
    "        history[\"Loss\"] = history[\"Loss\"] + [float(y) for y in content[1].strip().split(\",\")]\n",
    "        history[\"Date\"].append(content[0].split(\" Total Loss\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1e-3)/(1561/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 3e-06\n",
    "# optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "# train_step_fn = get_model_train_step_function(\n",
    "#     detection_model, optimizer, to_fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-06\n",
    "# optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "# train_step_fn = get_model_train_step_function(\n",
    "#     detection_model, optimizer, to_fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 學習率變化(手動調整)\n",
    "# (1e-3)/(4170/5)，1e-3 是 batch size : 5 、 images : 5 的 Adam 優化器最適學習率\n",
    "learning_rate = 5e-06\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "train_step_fn = get_model_train_step_function(\n",
    "    detection_model, optimizer, to_fine_tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_point = 0\n",
    "val_point = 0\n",
    "now_point = [\"file jpg\", 0]\n",
    "batch_size = 5\n",
    "total_size = round(len(name)/batch_size)\n",
    "now_loss = 0\n",
    "now_loss_total = 0\n",
    "epochs = 20\n",
    "train = data_generator(image_path, name, bbox, batch_size, random_image_process_v1)\n",
    "# train = default_data_generator(image_path, name, bbox, batch_size, image_process_default)\n",
    "gt_classes_list = [gt_classes for i in range(batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    for epoch in range(1, epochs+1):\n",
    "        now_loss = 0\n",
    "        now_loss_total = 0\n",
    "        with tqdm(total=total_size, ncols=100, ascii=' =') as pbar:\n",
    "            for one in range(total_size):\n",
    "                custom_train(batch_size, train)\n",
    "                now_loss_value = now_loss_total/now_loss\n",
    "                pbar.set_postfix_str(\"epoch %d, Total loss = %.7f\"%(epoch, now_loss_value))\n",
    "                pbar.update(1) \n",
    "        history[\"Loss\"].append(now_loss_value)\n",
    "        history[\"Date\"].append(str(datetime.now()).split(\".\")[0]+\",\")\n",
    "except:    \n",
    "    pbar.close()\n",
    "print(\"Finish\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_ = \"1e-03, 3e-04, 3e-05, 3e-06\"\n",
    "optimizer_ = \"SGD、Adam、RMSprop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"Date : %s \\noptimizer : %s \\nlearning_rate : %s  batch_size : %s  images : %s\"%(str(datetime.now()), optimizer_, learning_rate_, batch_size, len(name))\n",
    "plt.plot([i for i in range(1,len(history[\"Loss\"])+1)], history[\"Loss\"])\n",
    "plt.title(text)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Total Loss\")\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ckpt/history.txt\", \"w\", encoding=\"utf8\") as txt:\n",
    "    for e,r in zip(history[\"Date\"], history[\"Loss\"]):\n",
    "        txt.write(str(e)+\" Total Loss#\"+str(r)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/models/issues/8862#issuecomment-920330306\n",
    "detection_model\n",
    "new_pipeline_proto = config_util.create_pipeline_proto_from_configs(configs)\n",
    "config_util.save_pipeline_config(new_pipeline_proto, 'ckpt')\n",
    "\n",
    "exported_ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt_manager = tf.train.CheckpointManager(exported_ckpt, directory=\"ckpt\", max_to_keep=5)\n",
    "print('Done fine-tuning!')\n",
    "\n",
    "ckpt_manager.save()\n",
    "print('Checkpoint saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Done fine-tuning!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r\"C:\\Users\\sky66\\Downloads\\models\\research\\ckpt\\pipeline.config\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/exporter_main_v2.py\n",
    "# python C:\\Users\\sky66\\Downloads\\models\\research\\object_detection\\exporter_main_v2.py --pipeline_config_path C:\\Users\\sky66\\Downloads\\models\\research\\ckpt\\pipeline.config --trained_checkpoint_dir C:\\Users\\sky66\\Downloads\\models\\research\\ckpt --output_directory new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHlXL1x_Z3tc"
   },
   "source": [
    "# Load test images and run inference with new model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, uncomment this decorator if you want to run inference eagerly\n",
    "@tf.function(input_signature=[tf.TensorSpec(shape=[None,640,640,3], dtype=tf.float32)])\n",
    "def detect(input_tensor):\n",
    "    \"\"\"Run detection on an input image.\n",
    "    \n",
    "    Args:\n",
    "      input_tensor: A [1, height, width, 3] Tensor of type tf.float32.\n",
    "        Note that height and width can be anything since the image will be\n",
    "        immediately resized according to the needs of the model within this\n",
    "        function.\n",
    "    \n",
    "    Returns:\n",
    "      A dict containing 3 Tensors (`detection_boxes`, `detection_classes`,\n",
    "        and `detection_scores`).\n",
    "    \"\"\"\n",
    "    preprocessed_image, shapes = detection_model.preprocess(input_tensor)\n",
    "    prediction_dict = detection_model.predict(preprocessed_image, shapes)\n",
    "    return detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "# Note that the first frame will trigger tracing of the tf.function, which will\n",
    "# take some time, after which inference should be fast.    \n",
    "\n",
    "def img_to_tensor(img_np):\n",
    "    img_tf = tf.convert_to_tensor(img_np, dtype=tf.float32 )\n",
    "    img_tf = tf.expand_dims(img_tf, axis=0) \n",
    "    return img_tf \n",
    "\n",
    "def img_proccess(img):\n",
    "    img = Image.open(img)\n",
    "    \n",
    "    detection_img = img.resize((640,640))\n",
    "    detection_img = np.array(detection_img)\n",
    "    \n",
    "    img = np.array(img)\n",
    "    origin_img = np.zeros(img.shape)\n",
    "    \n",
    "    np.copyto(origin_img ,img)\n",
    "    \n",
    "    detection_img = img_to_tensor(detection_img)\n",
    "    \n",
    "    return {'origin_img_np':origin_img , 'detection_img_tensor':detection_img}\n",
    "\n",
    "def draw_bounding_box_on_image(image,\n",
    "                                 ymin,\n",
    "                                 xmin,\n",
    "                                 ymax,\n",
    "                                 xmax,\n",
    "                                 color = list(ImageColor.colormap.values())[0],\n",
    "                                 font = ImageFont.load_default(),\n",
    "                                 thickness=4,\n",
    "                                 display_str_list=()):\n",
    "    # image format RGB np.array (0~255)\n",
    "    \"\"\"Adds a bounding box to an image.\"\"\"\n",
    "    image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    im_width, im_height = image.size\n",
    "    (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n",
    "                                  ymin * im_height, ymax * im_height)\n",
    "    draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n",
    "               (left, top)],\n",
    "              width=thickness,\n",
    "              fill=color)\n",
    "    \n",
    "    # If the total height of the display strings added to the top of the bounding\n",
    "    # box exceeds the top of the image, stack the strings below the bounding box\n",
    "    # instead of above.\n",
    "    display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n",
    "    # Each display_str has a top and bottom margin of 0.05x.\n",
    "    total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n",
    "    \n",
    "    if top > total_display_str_height:\n",
    "        text_bottom = top\n",
    "    else:\n",
    "        text_bottom = top + total_display_str_height\n",
    "    # Reverse list and print from bottom to top.\n",
    "    for display_str in display_str_list[::-1]:\n",
    "        text_width, text_height = font.getsize(display_str)\n",
    "        margin = np.ceil(0.05 * text_height)\n",
    "        draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n",
    "                        (left + text_width, text_bottom)],\n",
    "                       fill=color)\n",
    "        draw.text((left + margin, text_bottom - text_height - margin),\n",
    "                  display_str,\n",
    "                  fill=\"black\",\n",
    "                  font=font)\n",
    "        text_bottom -= text_height - 2 * margin\n",
    "    return image  \n",
    "  \n",
    "def draw_boxes_s(\n",
    "         image, \n",
    "         class_names, \n",
    "         boxes,\n",
    "         scores,\n",
    "         score_limit,\n",
    "         max_box,\n",
    "         display = True):\n",
    "    # image format RGB np.array (0~255)\n",
    "    now_image_np = np.zeros((image.shape))\n",
    "    np.copyto(now_image_np , image)\n",
    "    \n",
    "    class_names = class_names.tolist()\n",
    "    boxes = boxes.tolist()\n",
    "    scores = scores.tolist()\n",
    "    \n",
    "    new_class_names = []\n",
    "    new_boxes = []\n",
    "    new_scores = []\n",
    "    \n",
    "    if max_box>len(boxes):\n",
    "        max_box = len(boxes)\n",
    "    \n",
    "    for e in range(max_box):\n",
    "        big = 0\n",
    "        for e in range(len(boxes)):\n",
    "            if scores[e]>scores[big]:\n",
    "                big = e\n",
    "        new_class_names.append(class_names.pop(big))\n",
    "        new_boxes.append(boxes.pop(big))\n",
    "        new_scores.append(scores.pop(big))     \n",
    "        \n",
    "    class_names = new_class_names\n",
    "    boxes = new_boxes\n",
    "    scores = new_scores\n",
    "    \n",
    "    for i in range(0,len(boxes)):\n",
    "        if(float(scores[i])>score_limit):\n",
    "            box , score , class_name = boxes[i] , scores[i] , class_names[i]\n",
    "            # print(box)\n",
    "            class_name = \"Person\"\n",
    "            colors = list(ImageColor.colormap.values())\n",
    "                  \n",
    "            font = ImageFont.load_default()\n",
    "            display_str = (class_name+\":\"+str(score))\n",
    "            color = colors[hash(class_name) % len(colors)]\n",
    "            \n",
    "            image_pil = Image.fromarray(np.uint8(now_image_np)).convert(\"RGB\")\n",
    "          \n",
    "            image_pil = draw_bounding_box_on_image(\n",
    "                image_pil,\n",
    "                *box,\n",
    "                list(ImageColor.colormap.values())[5],\n",
    "                ImageFont.load_default(),\n",
    "                display_str_list=[\"Person\"+\":\"+str(score)])\n",
    "            np.copyto(now_image_np, np.array(image_pil) )\n",
    "    if display:\n",
    "        ds( Image.fromarray(np.uint8( np.array(now_image_np) )).convert(\"RGB\") )    \n",
    "    return now_image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_path = r\"C:\\Users\\sky66\\fiftyone\\coco-2017\\raw\\nlp\"\n",
    "# test_id = 0\n",
    "# test_img = cv2.imread(test_path+\"\\\\\"+name[test_id], cv2.IMREAD_GRAYSCALE)\n",
    "# process_img, process_box = random_image_process(test_img, bbox[test_id])\n",
    "# draw_test = draw_bounding_box_on_image(process_img, *process_box, display_str_list=[\"Drawing Test\"])\n",
    "# ds(draw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ymin, xmin, ymax, xmax 範圍 0~1 (0%~100%)\n",
    "# # x座標是與圖片的左側相比\n",
    "# # y座標是與圖片的上側相比\n",
    "# # ymin, xmin 是圖片的左上角\n",
    "# # ymax, xmax 是圖片的右下角\n",
    "\n",
    "# test_img = r\"C:\\Users\\sky66\\fiftyone\\coco-2017\\raw\\train2017\\000000000436.jpg\"\n",
    "# origin_img = cv2.cvtColor(cv2.imread(test_img), cv2.COLOR_BGR2RGB)\n",
    "#                           ymin, xmin, ymax, xmax\n",
    "# ymin, xmin, ymax, xmax = [0.02, 0.02,  0.7, 0.9]\n",
    "# draw_test = draw_bounding_box_on_image(origin_img, ymin, xmin, ymax, xmax , display_str_list=[\"Drawing Test\"])\n",
    "# ds(draw_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_img = r\"aa.png\"\n",
    "predict_img = cv2.cvtColor(cv2.resize(img_to_array(load_img(test_img, color_mode = 'grayscale')), (640,640)), cv2.COLOR_GRAY2RGB)\n",
    "predict_img = tf.expand_dims(tf.convert_to_tensor(predict_img, dtype=tf.float32), axis=0)\n",
    "origin_img = cv2.cvtColor(cv2.imread(test_img), cv2.COLOR_BGR2RGB)\n",
    "detection_result = detect(predict_img)\n",
    "min_predict_score = 0.2\n",
    "max_view_box = 3\n",
    "predict_result = draw_boxes_s(\n",
    "                     origin_img,\n",
    "                     detection_result['detection_classes'][0].numpy(),\n",
    "                     detection_result['detection_boxes'][0].numpy(),\n",
    "                     detection_result[\"detection_scores\"][0].numpy(),\n",
    "                     min_predict_score,\n",
    "                     max_view_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = r\"C:\\Users\\sky66\\Downloads\\tumblr_464a8a1770ac6b2c09c80232911673b7_0306a7f3_1280.jpg\"\n",
    "predict_img = cv2.cvtColor(cv2.resize(cv2.imread(test_img, cv2.IMREAD_GRAYSCALE), (640,640)), cv2.COLOR_GRAY2RGB)\n",
    "predict_img = tf.expand_dims(tf.convert_to_tensor(predict_img, dtype=tf.float32), axis=0)\n",
    "origin_img = cv2.cvtColor(cv2.imread(test_img), cv2.COLOR_BGR2RGB)\n",
    "detection_result = detect(predict_img)\n",
    "min_predict_score = 0.2\n",
    "max_view_box = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predict_result = draw_boxes_s(\n",
    "                     origin_img,\n",
    "                     detection_result['detection_classes'][0].numpy(),\n",
    "                     detection_result['detection_boxes'][0].numpy(),\n",
    "                     detection_result[\"detection_scores\"][0].numpy(),\n",
    "                     min_predict_score,\n",
    "                     max_view_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MKBSnAN7o4t"
   },
   "outputs": [],
   "source": [
    "# tf.saved_model.save(\n",
    "#     detection_model , 'detection_model',\n",
    "#     signatures={\n",
    "#       'detection': detect.get_concrete_function()\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEstYegs3rmd"
   },
   "outputs": [],
   "source": [
    "# new_model = tf.saved_model.load('detection_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJrfjqsr4JeN"
   },
   "outputs": [],
   "source": [
    "# new_detection_model = new_model.signatures['detection']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "eager_few_shot_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
